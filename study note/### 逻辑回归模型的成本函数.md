### 逻辑回归模型的成本函数

#### 成本函数的作用

成本函数衡量模型预测结果与真实标签之间的差异。

#### 公式：成本函数定义

成本函数定义为：

\[ J(\hat{y}, y) = -y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \]

- 这里，\(\hat{y}\) 是模型预测的概率，\(y\) 是真实的标签（0 或 1）。

#### 目标

目标是使预测值 \(\hat{y}\) 尽可能接近真实标签 \(y\)。

#### 损失函数与成本函数

- **损失函数**：损失函数应用于单个训练样本。
- **成本函数**：成本函数是整个训练集的平均损失。

在训练逻辑回归模型时，目标是找到使成本函数最小化的参数 \( W \) 和 \( b \)。

#### 为什么这样设计这个函数

1. **概率性质**：
    - 逻辑回归输出的是一个概率值 \(\hat{y}\)，范围在 0 到 1 之间。
    - 真实标签 \(y\) 只有两个值：0 或 1。

2. **对数（log）函数的使用**：
    - 对数函数具有一些重要性质，使其非常适合衡量概率之间的差异。
    - 对数函数能够将概率的乘法转化为加法，简化计算。
    - 对数函数在概率接近 0 或 1 时能够放大误差，从而使模型更注重极端错误。

3. **损失函数的形式**：
    - 当 \(y = 1\) 时，成本函数为 \(-\log(\hat{y})\)：
      - 如果 \(\hat{y}\) 接近 1，\(\log(\hat{y})\) 是一个小的负数，\(-\log(\hat{y})\) 也是一个小的正数（成本低）。
      - 如果 \(\hat{y}\) 接近 0，\(\log(\hat{y})\) 是一个大的负数，\(-\log(\hat{y})\) 是一个大的正数（成本高）。
    - 当 \(y = 0\) 时，成本函数为 \(-\log(1 - \hat{y})\)：
      - 如果 \(\hat{y}\) 接近 0，\(\log(1 - \hat{y})\) 是一个小的负数，\(-\log(1 - \hat{y})\) 也是一个小的正数（成本低）。
      - 如果 \(\hat{y}\) 接近 1，\(\log(1 - \hat{y})\) 是一个大的负数，\(-\log(1 - \hat{y})\) 是一个大的正数（成本高）。

#### 什么是对数（log）

对数是数学中的一个运算，通常用于解决指数方程。对数函数的定义如下：

- 对数的基础公式是：如果 \(a^b = c\)，那么 \(\log_a(c) = b\)。
- 最常用的对数是自然对数（底数为 \(e\)，即 \(\log_e\) 或 \(\ln\)），和常用对数（底数为 10，即 \(\log_{10}\)）。

在逻辑回归中，通常使用自然对数 \(\log\)，它的底数是 \(e \approx 2.71828\)。

1. **自然对数的性质**：
    - \(\log(1) = 0\)：表示没有变化。
    - \(\log(x)\) 在 \(0 < x < 1\) 时为负，在 \(x > 1\) 时为正。
    - 当 \(x\) 越接近 0，\(\log(x)\) 的值越小（负值绝对值越大）。

2. **在成本函数中的应用**：
    - 对数函数能够将小的概率放大，使得模型更重视错误的预测。
    - 例如，当真实标签 \(y = 1\) 而 \(\hat{y}\) 非常小（接近 0）时，\(-\log(\hat{y})\) 会变得非常大，表示高成本。

### 结论

逻辑回归的成本函数设计成 \(-y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})\) 的原因是它能够有效评估模型预测概率与真实标签之间的差异，并通过对数函数放大极端错误，帮助模型更好地收敛。

---

### ロジスティック回帰モデルのコスト関数

#### コスト関数の役割

コスト関数はモデルの予測結果と実際のラベルとの違いを測定する。

#### 公式：コスト関数の定義

コスト関数は次のように定義される：

\[ J(\hat{y}, y) = -y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \]

- ここで、\(\hat{y}\) はモデルが予測した確率、\(y\) は実際のラベル（0 または 1）。

#### 目的

目的は予測値 \(\hat{y}\) を実際のラベル \(y\) にできるだけ近づけること。

#### 損失関数とコスト関数

- **損失関数**：損失関数は1つのトレーニングサンプルに適用される。
- **コスト関数**：コスト関数はトレーニングセット全体の平均損失。

ロジスティック回帰モデルをトレーニングする際の目的は、コスト関数を最小化するパラメータ \( W \) と \( b \) を見つけること。

#### なぜこのように設計されているのか

1. **確率の性質**：
    - ロジスティック回帰は確率値 \(\hat{y}\) を出力し、その範囲は0から1。
    - 実際のラベル \(y\) は0または1の2つの値のみ。

2. **対数（log）関数の使用**：
    - 対数関数には、確率間の違いを測定するのに適した重要な性質がある。
    - 対数関数は確率の乗算を加算に変換し、計算を簡単にする。
    - 確率が0や1に近づくときに誤差を拡大し、極端な誤りにモデルがより注目するようにする。

3. **損失関数の形式**：
    - \(y = 1\) の場合、コスト関数は \(-\log(\hat{y})\) となる：
      - \(\hat{y}\) が1に近い場合、\(\log(\hat{y})\) は小さな負の数であり、\(-\log(\hat{y})\) も小さな正の数（コストが低い）。
      - \(\hat{y}\) が0に近い場合、\(\log(\hat{y})\) は大きな負の数であり、\(-\log(\hat{y})\) は大きな正の数（コストが高い）。
    - \(y = 0\) の場合、コスト関数は \(-\log(1 - \hat{y})\) となる：
      - \(\hat{y}\) が0に近い場合、\(\log(1 - \hat{y})\) は小さな負の数であり、\(-\log(1 - \hat{y})\) も小さな正の数（コストが低い）。
      - \(\hat{y}\) が1に近い場合、\(\log(1 - \hat{y})\) は大きな負の数であり、\(-\log(1 - \hat{y})\) は大きな正の数（コストが高い）。

#### 対数（log）とは何か

対数は数学の演算の一つで、通常指数方程式を解くために使用される。対数関数は次のように定義される：

- 基本的な対数の公式：もし \(a^b = c\) ならば、\(\log_a(c) = b\)。
- 最も一般的な対数は自然対数（底が \(e\)、すなわち \(\log_e\) または \(\ln\)）と常用対数（底が10、すなわち \(\log_{10}\)）。

ロジスティック回帰では、通常自然対数 \(\log\) を使用し、その底は \(e \approx 2.71828\)。

1. **自然対数の性質**：
    - \(\log(1) = 0\)：変化がないことを示す。
    - \(\log(x)\) は \(0 < x < 1\) のとき負で、\(x > 1\) のとき正。
    - \(x\) が0に近づくと、\(\log(x)\) の値は非常に小さくなる（負の値の絶対値が大きくなる）。

2. **コスト関数での利用**：
    - 対数関数は小さな確率を拡大し、誤った予測に対してモデルがより注目するようにする。
    - 例えば、実際のラベル \(y = 1\) で \(\hat{y}\) が非常に小さい（0に近い）場合、\(-\log(\hat{y})\) は非常に大きくなり、高いコストを示す。

### 結論

ロジスティック回帰のコスト関数が \(-y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})\) と設計されている理由は、モデルの予測確率と実際のラベルの違いを効果的に評価し、対数関数によって極端な誤りを拡大し、モデルの収束を助けるためである。

---

### Logistic Regression Model Cost Function

#### Purpose of the Cost Function

The cost function measures the difference between the model's predicted results and the actual labels.

#### Formula: Definition of the Cost Function

The cost function is defined as:

\[ J(\hat{y}, y) = -y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \]

- Here, \(\hat{y}\) is the predicted probability by the model, and \(y\) is the actual label (0 or 1).

#### Objective

The objective is to make the predicted value \(\hat{y}\) as close as possible to the actual label \(y\).

#### Loss Function vs. Cost Function

- **Loss Function**: The loss function is applied to a single training sample.
- **Cost Function**: The cost function is the average loss over the entire training set.

The goal when training a logistic regression model is to find the parameters \( W \) and \( b \) that minimize the cost function.

#### Why the Function is Designed This Way

1. **Probability Nature**:
    - Logistic regression outputs a probability value \(\hat{y}\) that ranges from 0 to 1.
    - The actual label \(y\) has only two possible values: 0 or 1.

2. **Use of Logarithm (log) Function**:
    - The log function has some important properties that make it very suitable for measuring differences between probabilities.
    - The log function can convert the multiplication of probabilities into addition, simplifying calculations.
    - The log function can amplify errors when probabilities are close to 0 or 1, making the model pay more attention to extreme errors.

3. **Form of the Loss Function**:
    - When \(y = 1\), the cost function is \(-\log(\hat{y})\):
      - If \(\hat{y}\) is close to 1, \(\log(\hat{y})\) is a small negative number, and \(-\log(\hat{y})\) is also a small positive number (low cost).
      - If \(\hat{y}\) is close to 0, \(\log(\hat{y})\) is a large negative number, and \(-\log(\hat{y})\) is a large positive number (high cost).
    - When \(y = 0\), the cost function is \(-\log(1 - \hat{y})\):
      - If \(\hat{y}\) is close to 0, \(\log(1 - \hat{y})\) is a small negative number, and \(-\log(1 - \hat{y})\) is also a small positive number (low cost).
      - If \(\hat{y}\) is close to 1, \(\log(1 - \hat{y})\) is a large negative number, and \(-\log(1 - \hat{y})\) is a large positive number (high cost).

#### What is a Logarithm (log)

A logarithm is a mathematical operation used to solve exponential equations. The log function is defined as follows:

- The basic logarithm formula is: if \(a^b = c\), then \(\log_a(c) = b\).
- The most common logarithms are the natural logarithm (base \(e\), i.e., \(\log_e\) or \(\ln\)), and the common logarithm (base 10, i.e., \(\log_{10}\)).

In logistic regression, the natural logarithm \(\log\) is usually used, with a base of \(e \approx 2.71828\).

1. **Properties of the Natural Logarithm**:
    - \(\log(1) = 0\): Indicates no change.
    - \(\log(x)\) is negative when \(0 < x < 1\) and positive when \(x > 1\).
    - The closer \(x\) is to 0, the smaller (more negative) \(\log(x)\) becomes.

2. **Application in the Cost Function**:
    - The log function can amplify small probabilities, making the model pay more attention to incorrect predictions.
    - For example, when the actual label \(y = 1\) and \(\hat{y}\) is very small (close to 0), \(-\log(\hat{y})\) becomes very large, indicating high cost.

### Conclusion

The reason the cost function in logistic regression is designed as \(-y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})\) is that it effectively evaluates the difference between the model's predicted probabilities and the actual labels, and by using the log function to amplify extreme errors, it helps the model converge better.