### 逻辑回归的梯度下降与计算图解释

在逻辑回归中，我们通过梯度下降优化模型参数，以最小化损失函数。视频中详细讲解了如何使用计算图来计算导数，从而实现梯度下降。

#### 逻辑回归模型

假设我们有一个逻辑回归模型，其输出为预测值 \( \hat{y} \)，定义如下：

\[ \hat{y} = \sigma(z) \]

其中，\( \sigma(z) \) 是 sigmoid 函数：

\[ \sigma(z) = \frac{1}{1 + e^{-z}} \]

而 \( z \) 定义为输入特征的线性组合：

\[ z = w_1 x_1 + w_2 x_2 + b \]

#### 损失函数

对于单个样本，损失函数 \( L \) 定义为：

\[ L(a, y) = - [ y \log(a) + (1 - y) \log(1 - a) ] \]

其中，\( a = \hat{y} \) 是模型的预测值，\( y \) 是真实标签。

### 使用计算图实现梯度计算

计算图是一个直观工具，用于表示计算过程并辅助反向传播计算导数。在这个例子中，我们通过计算图来计算损失函数对模型参数 \( w_1 \)、\( w_2 \) 和 \( b \) 的导数。

#### 前向传播

1. **输入特征和参数**：
   - \( x_1 \), \( x_2 \): 输入特征
   - \( w_1 \), \( w_2 \), \( b \): 模型参数

2. **计算 \( z \)**：
   \[ z = w_1 x_1 + w_2 x_2 + b \]

3. **计算 \( \hat{y} \) (即 \( a \))**：
   \[ \hat{y} = \sigma(z) \]

4. **计算损失 \( L \)**：
   \[ L(a, y) = - [ y \log(a) + (1 - y) \log(1 - a) ] \]

#### 反向传播

1. **计算 \( \frac{\partial L}{\partial a} \)**：
   \[ \frac{\partial L}{\partial a} = - \frac{y}{a} + \frac{1 - y}{1 - a} \]

2. **计算 \( \frac{\partial a}{\partial z} \)**：
   \[ \frac{\partial a}{\partial z} = a (1 - a) \]

3. **计算 \( \frac{\partial L}{\partial z} \)** 使用链式法则：
   \[ \frac{\partial L}{\partial z} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} = (a - y) \]

4. **计算损失函数对参数 \( w_1 \)、\( w_2 \) 和 \( b \) 的导数**：
   - 对 \( w_1 \):
     \[ \frac{\partial L}{\partial w_1} = x_1 \cdot \frac{\partial L}{\partial z} = x_1 \cdot (a - y) \]
   - 对 \( w_2 \):
     \[ \frac{\partial L}{\partial w_2} = x_2 \cdot \frac{\partial L}{\partial z} = x_2 \cdot (a - y) \]
   - 对 \( b \):
     \[ \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} = a - y \]

#### 参数更新

使用梯度下降更新参数：

\[ w_1 = w_1 - \alpha \cdot \frac{\partial L}{\partial w_1} \]
\[ w_2 = w_2 - \alpha \cdot \frac{\partial L}{\partial w_2} \]
\[ b = b - \alpha \cdot \frac{\partial L}{\partial b} \]

其中，\( \alpha \) 是学习率。

### 总结

通过使用计算图，我们可以系统地进行前向传播和反向传播，逐步计算每个变量对损失函数的影响。在逻辑回归中，这种方法使得我们能够有效地计算梯度，从而更新模型参数，最终实现模型的优化。这样的方法不仅适用于逻辑回归，也为学习深度神经网络的梯度计算打下基础。