
#### 前向传播
前向传播是从输入到输出的计算过程。以一个简单的神经网络为例，其中输入变量是 \( x \)，权重是 \( w \)，偏置是 \( b \)，输出是 \( y \)。

1. 输入 \( x \)。
2. 计算 \( z = wx + b \)。
3. 使用激活函数（例如 Sigmoid 函数）计算输出 \( y = \sigma(z) \)，其中 \( \sigma(z) = \frac{1}{1 + e^{-z}} \)。

#### 反向传播
反向传播是从输出到输入的梯度计算过程，用于更新参数以最小化损失函数 \( L \)。

假设损失函数 \( L \) 为均方误差：

\[ L = \frac{1}{2} (y_{\text{pred}} - y_{\text{true}})^2 \]

步骤如下：

1. 计算损失 \( L \)。
2. 计算梯度：
    \[ \frac{\partial L}{\partial y_{\text{pred}}} = y_{\text{pred}} - y_{\text{true}} \]
    \[ \frac{\partial L}{\partial z} = \frac{\partial L}{\partial y_{\text{pred}}} \cdot \sigma'(z) \]
    \[ \frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot x \]
    \[ \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} \]
3. 使用梯度下降法更新参数：
    \[ w \leftarrow w - \eta \frac{\partial L}{\partial w} \]
    \[ b \leftarrow b - \eta \frac{\partial L}{\partial b} \]

#### 计算图示例

```mermaid
graph TD;
    A[x] --> B[z = wx + b];
    B --> C[y = σ(z)];
    C --> D[L = (y_pred - y_true)^2 / 2];

    subgraph Forward
        A;
        B;
        C;
    end

    subgraph Backward
        D --> C;
        C --> B;
        B --> A;
    end
```

#### 前向传播和反向传播的关系
- **前向传播**：从输入开始计算输出。
- **反向传播**：从输出开始计算梯度，并更新参数。

---

### 日语

#### 前方伝播
前方伝播は、入力から出力までの計算プロセスです。単純なニューラルネットワークを例にとると、入力変数は \( x \)、重みは \( w \)、バイアスは \( b \)、出力は \( y \) です。

1. 入力 \( x \)。
2. 計算 \( z = wx + b \)。
3. 活性化関数（例えばシグモイド関数）を使用して出力 \( y = \sigma(z) \) を計算します。ここで、 \( \sigma(z) = \frac{1}{1 + e^{-z}} \) です。

#### 逆伝播
逆伝播は、出力から入力までの勾配を計算し、損失関数 \( L \) を最小化するためにパラメータを更新するプロセスです。

損失関数 \( L \) が平均二乗誤差であると仮定します：

\[ L = \frac{1}{2} (y_{\text{pred}} - y_{\text{true}})^2 \]

手順は以下の通りです：

1. 損失 \( L \) を計算します。
2. 勾配を計算します：
    \[ \frac{\partial L}{\partial y_{\text{pred}}} = y_{\text{pred}} - y_{\text{true}} \]
    \[ \frac{\partial L}{\partial z} = \frac{\partial L}{\partial y_{\text{pred}}} \cdot \sigma'(z) \]
    \[ \frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot x \]
    \[ \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} \]
3. 勾配降下法を使用してパラメータを更新します：
    \[ w \leftarrow w - \eta \frac{\partial L}{\partial w} \]
    \[ b \leftarrow b - \eta \frac{\partial L}{\partial b} \]

#### 計算グラフの例

```mermaid
graph TD;
    A[x] --> B[z = wx + b];
    B --> C[y = σ(z)];
    C --> D[L = (y_pred - y_true)^2 / 2];

    subgraph Forward
        A;
        B;
        C;
    end

    subgraph Backward
        D --> C;
        C --> B;
        B --> A;
    end
```

#### 前方伝播と逆伝播の関係
- **前方伝播**：入力から出力までを計算します。
- **逆伝播**：出力から勾配を計算し、パラメータを更新します。

---

### English

#### Forward Propagation
Forward propagation is the process of calculating the output from the input. For a simple neural network, the input variable is \( x \), the weight is \( w \), the bias is \( b \), and the output is \( y \).

1. Input \( x \).
2. Compute \( z = wx + b \).
3. Use the activation function (e.g., Sigmoid function) to compute the output \( y = \sigma(z) \), where \( \sigma(z) = \frac{1}{1 + e^{-z}} \).

#### Backward Propagation
Backward propagation is the process of calculating the gradient from the output to the input, used to update parameters to minimize the loss function \( L \).

Assuming the loss function \( L \) is the mean squared error:

\[ L = \frac{1}{2} (y_{\text{pred}} - y_{\text{true}})^2 \]

The steps are as follows:

1. Calculate the loss \( L \).
2. Compute the gradients:
    \[ \frac{\partial L}{\partial y_{\text{pred}}} = y_{\text{pred}} - y_{\text{true}} \]
    \[ \frac{\partial L}{\partial z} = \frac{\partial L}{\partial y_{\text{pred}}} \cdot \sigma'(z) \]
    \[ \frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot x \]
    \[ \frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} \]
3. Use gradient descent to update the parameters:
    \[ w \leftarrow w - \eta \frac{\partial L}{\partial w} \]
    \[ b \leftarrow b - \eta \frac{\partial L}{\partial b} \]

#### Example Computation Graph

```mermaid
graph TD;
    A[x] --> B[z = wx + b];
    B --> C[y = σ(z)];
    C --> D[L = (y_pred - y_true)^2 / 2];

    subgraph Forward
        A;
        B;
        C;
    end

    subgraph Backward
        D --> C;
        C --> B;
        B --> A;
    end
```

#### Relationship Between Forward and Backward Propagation
- **Forward Propagation**: Calculates the output starting from the input.
- **Backward Propagation**: Calculates the gradients starting from the output and updates the parameters.